---
- name: Check for required variables
  assert:
    msg: |
      The log artifact source must be provided.
    that:
      - "artifact.source is defined"

# TODO(odyssey4me):
# Perhaps have these facts provided by variables
# defined in groovy instead so that we do not have
# to rely on environment variables. This may make
# the switch to declaritive pipelines easier.
- name: Set dependent facts
  set_fact:
    # 30 days
    artifacts_expire_after: 2592000
    job_name: "{{ lookup('env', 'JOB_NAME') }}"
    build_number: "{{ lookup('env', 'BUILD_NUMBER') }}"

- name: Check that the JOB_NAME and BUILD_NUMBER have been provided
  debug:
    msg: |
      The JOB_NAME and BUILD_NUMBER environment variables must be set. Skipping artifact upload.
  when:
    - "(job_name == '') or (build_number == '')"

- name: Check if the source exists
  stat:
    path: "{{ artifact.source }}"
    follow: yes
    get_attributes: no
    get_checksum: no
    get_md5: no
    get_mime: no
  register: _artifact_path

- name: Show whether the source exists
  debug:
    msg: "source {{ artifact.source }} exists: {{ _artifact_path.stat.exists | bool }}"

- name: Upload the artifacts if all conditions are met
  when:
    - "_artifact_path.stat.exists | bool"
    - "job_name != ''"
    - "build_number != ''"
  block:
    - name: Set the archive base name and object container name
      set_fact:
        archive_base_name: "{{ job_name }}_{{ build_number }}"
        object_store_container_name: "jenkinsjob_{{ job_name }}_{{ build_number }}"

    - name: Move artifacts dir so that downloaded archives have a useful name
      shell: mv "{{ artifact['source'] | basename }}" "{{ archive_base_name }}"
      args:
        chdir: "{{ artifact['source'] | dirname }}"

    - name: Set the archive directory
      set_fact:
        adir: "{{ artifact['source'] | dirname }}/{{ archive_base_name }}"

    # Links are unlikely to work when expanded on a different
    # system so remove them. Create a list of removed links
    # to give users a clue as to why their file is missing.
    # Also swift client fails on dangling links.
    - name: Remove links from artifacts
      shell: |
        find {{ adir | basename }} \
          -type l \
          -print \
          -delete \
          | tee {{ adir | basename }}/removedlinks.txt
      args:
        chdir: "{{ adir | dirname }}"
      register: removed_links

    # This task runs async while the individual files are being uploaded
    # then when compression is complete, the resultant archive is uploaded.
    # 3 Hour Timeout
    - name: Create archive asynchronously
      shell: "tar -c {{ adir | basename }} | gzip --fast > {{ archive_base_name }}.tar.gz"
      args:
        chdir: "{{ adir | dirname }}"
      async: 10800
      poll: 0
      register: create_archive_async
      tags:
        - skip_ansible_lint

    - include: cloudfiles_create_container.yml

    # Do this before generating index.html, styles.css and data.json
    # So they don't show up on the final page.
    - name: Generate file list with sizes for index page data
      shell: |
        find . \
          -type f \
          -mindepth 1 \
          -exec ls -l {} \; \
        |sed 's+\./++g'
      args:
        chdir: "{{ adir }}"
      register: file_list

    - name: Copy index.html
      copy:
        src: templates/artifact/index.html
        dest: "{{ adir }}/index.html"

    - name: Copy style.css
      copy:
        src: templates/artifact/styles.css
        dest: "{{ adir }}/styles.css"

    # The ansible os_object module does not currently support setting
    # the object expiration header field, nor does it do threaded uploads
    # (which make this much faster), so we use the swift client instead.
    # Checksum validation is also disabled to improve upload speed.
    - name: Upload Artifacts to Cloud Files
      command: >-
        swift upload {{ object_store_container_name }} {{ adir | basename }}
        --object-threads 100
        --ignore-checksum
        --header 'X-Delete-After:{{ artifacts_expire_after }}'
      args:
        chdir: "{{ adir | dirname }}"
      environment:
        OS_AUTH_TOKEN: "{{ auth_token }}"
        OS_STORAGE_URL: "{{ (object_store['endpoints'] | selectattr('region', 'equalto', region) | first)['publicURL'] }}"
      # TODO(odyssey4me):
      # Consider removing '--ignore-checksum' and adding '--skip-identical'
      # and/or '--changed', then removing the 'failed_when' below and adding
      # retry/until to improve the chances of success.
      failed_when: false
      register: artifact_upload

    # this is json data containing metadata and a list of files
    # that will be downloaded by index.html
    # and used to display a list of available artifacts
    - name: Generate data.json
      copy:
        content: |
          {
            "job_name": "{{ job_name }}",
            "build_number": "{{ build_number }}",
            "archive_base_name": "{{ archive_base_name }}",
            "container_public_url": "{{ container_public_url}}",
            "removed_links_count": {{ removed_links.stdout_lines|count }},
            "failed_uploads": [
              {% for line in artifact_upload.stderr_lines %}
                "{{ line }}"{{ "," if not loop.last else "" }}
              {% endfor %}
            ],
            "files": [
              {% for file_line in file_list.stdout_lines %}
                {% set file_list = file_line.split() %}
                {% if file_list|length > 3 %}
                  {
                    "path": "{{file_list[-1]}}",
                    "size": {{file_list[4]}}
                  }{{ "," if not loop.last else "" }}
                {% endif %}
              {% endfor %}
            ],
            "archives": [
              "{{ container_public_url }}/{{ archive_base_name }}.tar.gz"
            ]
          }
        dest: "{{ adir }}/data.json"

    # The ansible os_object module does not currently support setting
    # the object expiration header field, nor does it do threaded uploads
    # (which make this much faster), so we use the swift client instead.
    - name: Upload index data (data.json) to Cloud Files
      command: >-
        swift upload {{ object_store_container_name }} {{ adir | basename }}/data.json
        --header 'X-Delete-After: {{ artifacts_expire_after }}'
      args:
        chdir: "{{ adir | dirname }}"
      environment:
        OS_AUTH_TOKEN: "{{ auth_token }}"
        OS_STORAGE_URL: "{{ (object_store['endpoints'] | selectattr('region', 'equalto', region) | first)['publicURL'] }}"
      register: upload_data
      until: upload_data | success
      retries: 10
      delay: 30

    - name: Wait for async archive creation to complete
      async_status:
        jid: "{{ create_archive_async.ansible_job_id }}"
      register: caa
      until: caa.finished
      retries: 180
      delay: 60

    # The ansible os_object module does not currently support setting
    # the object expiration header field, nor does it do threaded uploads
    # (which make this much faster), so we use the swift client instead.
    - name: Upload archive to Cloud Files
      command: >-
        swift upload {{ object_store_container_name }} {{ archive_base_name }}.tar.gz
        --header 'X-Delete-After: {{ artifacts_expire_after }}'
      args:
        chdir: "{{ adir | dirname }}"
      environment:
        OS_AUTH_TOKEN: "{{ auth_token }}"
        OS_STORAGE_URL: "{{ (object_store['endpoints'] | selectattr('region', 'equalto', region) | first)['publicURL'] }}"
      register: upload_archive
      until: upload_archive | success
      retries: 10
      delay: 30

    - name: "Check if build_artifacts.yml exists"
      stat:
        path: "{{ lookup('env', 'WORKSPACE') }}/build_artifacts.yml"
      register: build_artifacts_stat

    - name: "Load published artefact info for this build"
      include_vars:
        file: "{{ lookup('env', 'WORKSPACE') }}/build_artifacts.yml"
        name: "existing_build_artifacts"
      when: build_artifacts_stat.stat.exists

    - name: "Update build_artifacts with log artefacts"
      set_fact:
        new_build_artifacts: "{{ {'artifacts': {'log': {'name': 'build-logs', 'container_name': '{{ object_store_container_name }}', 'public_url': '{{ container_public_url }}/{{ archive_base_name }}/index.html', 'title': 'Build log artifacts'}}} }}"

    - name: "Update published artefacts file"
      copy:
        content: "{{ existing_build_artifacts | default({}) | combine(new_build_artifacts, recursive=True) | to_nice_yaml }}"
        dest: "{{ lookup('env', 'WORKSPACE') }}/build_artifacts.yml"
