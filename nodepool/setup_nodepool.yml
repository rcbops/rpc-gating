---
# We need to target all hosts in the group first to gather facts,
# otherwise any tasks using the group host_vars will fail.
- name: Gather facts from nodepool_server group
  hosts: "nodepool_server"
  gather_facts: no
  user: root
  tasks:
    - name: Wait for a successful connection
      wait_for_connection:
        connect_timeout: 2
        delay: 5
        sleep: 5
        timeout: 60

    - name: Gather facts
      setup:
        gather_subset: "!facter,!ohai"

- name: Setup zookeeper servers serially
  hosts: "nodepool_server"
  gather_facts: no
  user: root
  # NOTE(odyssey4me):
  # The zookeeper service loses all its data when it restarts,
  # which may happen when packages are upgraded. To ensure that
  # this is not an issue for us we execute the playbook in
  # serial so that the cluster always retains its quorum.
  serial: 1
  vars:
    zookeeper_debian_apt_install: yes
    zookeeper_hosts: "{{ groups['nodepool_server'] }}"
    zookeeper_access_list:
      - 74.205.74.100 # CIT Jenkins master (NAT)
    logrotate_configs:
      - name: zookeeper
        log: /var/log/zookeeper/zookeeper.log
        options:
          - compress
          - missingok
          - rotate 7
          - daily
          - notifempty
      - name: zookeeper_cron_cleanup
        log: /var/log/zookeeper/zookeeper_cron_cleanup.log
        options:
          - compress
          - missingok
          - rotate 7
          - weekly
          - notifempty
  handlers:
    - name: Restart zookeeper
      service:
        name: zookeeper
        daemon_reload: yes
        state: restarted

  tasks:
    - name: Install prerequisite distro packages
      apt:
        name:
          # requirement for zookeeper
          - openjdk-8-jre-headless
          # requirements for ansible
          # TODO(odyssey4me):
          # Remove aptitude when we shift to using
          # ansible 2.4
          - aptitude
          - python3-apt
          - python-minimal
          - python3-minimal
        update_cache: yes

    # the default /etc/hosts file results in the name of the instance
    # resolving to its own private address first, causing zookeeper
    # to listen on the wrong address, and thus clustering to fail
    - name: Prepare /etc/hosts for the zookeeper group
      copy:
        content: |
          127.0.0.1 localhost
          # The following lines are desirable for IPv6 capable hosts
          ::1 ip6-localhost ip6-loopback
          fe00::0 ip6-localnet
          ff00::0 ip6-mcastprefix
          ff02::1 ip6-allnodes
          ff02::2 ip6-allrouters
          ff02::3 ip6-allhosts
          # zookeeper hosts
          {% for host in zookeeper_hosts | sort %}
          {{ hostvars[host].ansible_default_ipv4.address }} {{ host }}
          {{ hostvars[host].ansible_default_ipv6.address }} {{ host }}
          {% endfor %}
        dest: "/etc/hosts"
      notify:
        - Restart zookeeper

    - name: Configure firewall to allow cluster traffic (ipv4)
      ufw:
        rule: allow
        from_ip: "{{ hostvars[item].ansible_default_ipv4.address }}"
      loop: "{{ zookeeper_hosts }}"
      notify:
        - Restart zookeeper

    - name: Configure firewall to allow access to zookeeper from additional hosts
      ufw:
        rule: allow
        from_ip: "{{ item }}"
      loop: "{{ zookeeper_access_list }}"
      notify:
        - Restart zookeeper

    - name: Configure firewall to allow cluster traffic (ipv6)
      ufw:
        rule: allow
        from_ip: "{{ hostvars[item].ansible_default_ipv6.address }}"
      loop: "{{ zookeeper_hosts }}"
      notify:
        - Restart zookeeper

    - name: Setup zookeeper
      include_role:
        name: AnsibleShipyard.zookeeper

    - name: Setup log rotation
      include_role:
        name: openstack.logrotate

    - name: Zookeeper TX Log Cleanup Cron
      cron:
        name: "zookeeper cleanup"
        minute: 0
        hour: 0
        user: root
        job: "/usr/share/zookeeper/bin/zkCleanup.sh -n 3 >> /var/log/zookeeper/zookeeper_cron_cleanup.log 2>&1"
        cron_file: "zookeeper_cleanup"

- name: Setup nodepool on the first server in the group
  hosts: "nodepool_server[0]"
  gather_facts: no
  user: root
  vars:
    nodepool_file_nodepool_yaml_src: "{{ lookup('env', 'WORKSPACE') }}/rpc-gating/nodepool/templates/nodepool.yml.j2"
    nodepool_git_version: 67824d8e64250ede0cb4a3ebe344412f5cba2218 # HEAD of 'master' as of 25 July 2018
    nodepool_pip_executable: pip3
    nodepool_file_nodepool_builder_service_config_src: systemd-service-override.conf.j2
    nodepool_file_nodepool_launcher_service_config_src: systemd-service-override.conf.j2

    diskimage_builder_git_version: "2.12.1" # Current release as of 22 Mar 2018
    diskimage_builder_elements_src: "elements"

    logrotate_configs:
      - name: nodepool-builder
        log: /var/log/nodepool/builder-debug.log /var/log/nodepool/nodepool-builder.log
        options:
          - compress
          - missingok
          - rotate 7
          - daily
          - notifempty
      - name: nodepool-launcher
        log: /var/log/nodepool/launcher-debug.log /var/log/nodepool/nodepool-launcher.log
        options:
          - compress
          - missingok
          - rotate 7
          - daily
          - notifempty

  handlers:
    - name: Restart nodepool-builder
      service:
        name: nodepool-builder
        daemon_reload: yes
        state: restarted
      listen: Restart all services

    - name: Restart nodepool-launcher
      service:
        name: nodepool-launcher
        daemon_reload: yes
        state: restarted
      listen: Restart all services

    - name: Trigger new diskimage build
      shell: |
        nodepool dib-image-list |awk '/+|Image/{next}; {a[$4]=$4} END{for (i in a){ print i}}'|while read i; do
          nodepool image-build $i
        done
      become: yes
      become_user: nodepool

  tasks:
    - name: Install prerequisite distro packages
      apt:
        name:
          # requirement for cloning the repos
          # we install from
          - git
          # requirements for pip installs
          - gcc
          - libffi-dev
          - libssl-dev
          - python-pip
          - python-virtualenv
          # requirements for ansible
          # TODO(odyssey4me):
          # Remove aptitude when we shift to using
          # ansible 2.4
          - aptitude
          - python3-apt
          - python-minimal
          - python3-minimal
          - python3-pip
        update_cache: yes

    - name: Install prerequisite pip packages
      pip:
        name:
          - rackspaceauth
        executable: pip3
      notify:
        - Restart nodepool-launcher
        - Restart nodepool-builder

    - name: Ensure nodepool is not installed under python2.x
      pip:
        name: nodepool
        state: absent
        executable: pip2
      notify:
        - Restart nodepool-launcher
        - Restart nodepool-builder

    # install extra packages for diskimage-builder
    # The native xenial package which provides vhd-utils (blktap-utils) does not support
    # the 'convert' command which is required in order to properly prepare VHD images for
    # the Xen hosts used by Rackspace Public Cloud. We therefore make use of the same PPA
    # used by openstack-infra which has the modified version available.
    # https://launchpad.net/~openstack-ci-core/+archive/ubuntu/vhd-util
    # built from: https://github.com/emonty/vhd-util
    # deployed by: https://github.com/openstack-infra/puppet-diskimage_builder/blob/339340409823927bb987f0195c6cedfdace05f4a/manifests/init.pp#L26

    - name: Add vhd-util PPA
      apt_repository:
        filename: "vhd-util"
        repo: "ppa:openstack-ci-core/vhd-util"
        update_cache: yes
      notify:
        - Restart nodepool-builder

    - name: Install vhd-util
      apt:
        name: "vhd-util"
      notify:
        - Restart nodepool-builder

    - name: Setup diskimage-builder
      include_role:
        name: openstack.diskimage-builder

    # NOTE(odyssey4me):
    # We execute the pre-tasks for the role to create the nodepool
    # user/group so that we can put the various files in need in-place
    # before we setup all the services.
    - name: Create nodepool user directories
      include_role:
        name: openstack.nodepool
      vars:
        nodepool_task_manager:
          - pre

    - name: Create openstack config directory
      file:
        path: "/var/lib/nodepool/.config/openstack"
        owner: "nodepool"
        group: "nodepool"
        mode: "0700"
        state: directory

    - name: Copy clouds.yaml
      copy:
        src: "{{ lookup('env', 'OS_CLIENT_CONFIG_FILE') }}"
        dest: "/var/lib/nodepool/.config/openstack/clouds.yaml"
        owner: "nodepool"
        group: "nodepool"
        mode: "0600"
      notify:
        - Restart nodepool-builder
        - Restart nodepool-launcher

    - name: Create ssh config directory
      file:
        path: "/home/nodepool/.ssh"
        owner: "nodepool"
        group: "nodepool"
        mode: "0700"
        state: directory

    - name: Copy private key
      copy:
        src: "{{ lookup('env', 'JENKINS_SSH_PRIVKEY') }}"
        dest: "/home/nodepool/.ssh/id_rsa"
        owner: "nodepool"
        group: "nodepool"
        mode: "0600"
      notify:
        - Restart nodepool-launcher

    - name: Allow passwordless sudo for nodepool
      lineinfile:
        dest: /etc/sudoers.d/nodepool
        create: yes
        state: present
        regexp: '^%nodepool'
        line: '%nodepool ALL=NOPASSWD: ALL'
        validate: visudo -cf %s
      notify:
        - Restart nodepool-builder

    - name: Create diskimage-builder tmp directory
      file:
        path: "/opt/nodepool/dib_tmp"
        owner: "nodepool"
        group: "nodepool"
        state: directory
      notify:
        - Restart nodepool-builder

    - name: Copy the diskimage-builder elements
      synchronize:
        src: "{{ diskimage_builder_elements_src }}"
        dest: "/opt/nodepool/"
        delete: yes
        rsync_opts:
          - "--chown=nodepool:nodepool"
      notify:
        - Restart nodepool-builder
        - Trigger new diskimage build

    - name: Setup nodepool services
      include_role:
        name: openstack.nodepool

    - name: Setup log rotation
      include_role:
        name: openstack.logrotate
