- job:
    name: 'Periodic-Cleanup'
    project-type: workflow
    parameters:
      - string:
          name: "INSTANCE_AGE_LIMIT"
          default: "12"
          description: |
            Hours. Instances older than this will be removed.
      - string:
          name: "PROTECTED_PREFIX"
          default: "long-running-slave|influx-|WEBHOOK-PROXY|webhook-proxy|nodepool|repo-server-|ArtifactBuilder|statsd-server-|graphite-server|grafana-server"
          description: |
            Instances that match this prefix regex will not be cleaned up.
      - string:
          name: "REGIONS"
          default: "DFW IAD ORD HKG SYD"
          description: |
            Only instances in the specified region will be cleaned up.
      - rpc_gating_params
    triggers:
      - timed: |
          H * * * 2-7
          H 13-23 * * 1
    properties:
      - build-discarder:
          days-to-keep: 3
    dsl: |
      library "rpc-gating@${RPC_GATING_BRANCH}"

      // Get list of jenkins slaves
      @NonCPS
      def getLongRunningNodes() {
        return jenkins.model.Jenkins.instance.nodes.grep {
          // Only return nodes whose name starts with one
          // of these expressions. Single use slaves won't match
          // so are filtered out.
          node -> node.name =~ /^(long-|rpc-jenkins-n)/
        }.collect {
          // node objects are not serializable so return a list
          // of names instead :(
          node -> node.name
        // master isn't in jenkins.model.Jenkins.instance.nodes so
        // add it to the filtered list.
        } + "master"
      }

      // end of functions

      common.globalWraps(){
        // run node cleanups on all nodes
        def nodes = getLongRunningNodes()
        def parallel_steps = [:]
        for (int i=0; i<nodes.size(); i++){
          def nodeName = nodes[i]
          parallel_steps['node_'+nodeName] = {
            common.use_node(nodeName){
              try {
                stage("Cleanup "+nodeName){
                  dir("rpc-gating") {
                    sh "scripts/workspace_cleanup.sh"
                    sh "scripts/tmp_cleanup.sh"
                    sh "scripts/docker_cleanup.sh"
                  }
                }
              } catch(e) {
                print(e)
                common.build_failure_issue("RE")
                throw e
              }
            }
          }
        }
        parallel parallel_steps

        // run the pubcloud and jenkins node cleanup only on an internal slave
        common.internal_slave(){
          try {
            dir("rpc-gating") {
              stage("Docker Build"){
                  common.docker_cache_workaround()
                  container = docker.build env.BUILD_TAG.toLowerCase()
              }
              // container.inside is a docker function that does
              // not run the same initialisaiton steps as
              // common.use_node/shared_slave/internal_slave.
              container.inside {
                stage("Docker Checkout"){
                  git branch: env.RPC_GATING_BRANCH, url: "https://github.com/rcbops/rpc-gating"
                }
                stage("Public Cloud Cleanup"){
                  common.withRequestedCredentials("cloud_creds, jenkins_api_creds") {
                    clouds_cfg = common.writeCloudsCfg()
                    withEnv(["OS_CLIENT_CONFIG_FILE=${clouds_cfg}"]){
                      sh "python scripts/periodic_cleanup.py"
                    }
                  }
                }
              }
            }
          } catch(e) {
            print(e)
            common.build_failure_issue("RE")
            throw e
          }
        } // internal_slave
        stage("Master Log Compression"){
          node("master"){
            // Note that already compressed logs will be skipped as they
            // get renamed .gz .
            // Logs newer than 1 day are skipped to prevent compressing
            // the logs from running builds. Unfortunately the core code
            // which enables jenkins to read compressed logs has a bug,
            // so we've had to set compression to only happen after 5
            // days. ref RE-2215
            sh """#!/bin/bash
              cd /var/lib/jenkins/jobs
              while read stagelog; do
                  # If we encountered the cooresponding gzip file, then the prior gzip operation on this file
                  # was interruped or failed for some reason. Note the error, clean up, and move on.
                  if [[ -f \$stagelog.gz ]]; then
                    echo "Both \$stagelog and \$stagelog.gz exist. Was gzip previously interrupted? Removing \$stagelog.gz..."
                    rm -f \$stagelog.gz
                  fi
                  echo "Compressing log: \$stagelog..."
                  gzip \$stagelog
                  _rc=\$?
                  if [ \$_rc -ne '0' ]; then
                    echo "gzip file \$stagelog failed with a non-zero response code of: \$_rc. Setting job stage response code to: \$_rc. This will fail the stage."
                    rc=\$_rc
                  fi
              done < <(find . -regex '.+/builds/[0-9]+/\\([0-9]+\\.\\)?log' -mtime +5)
              exit \${rc:-0}
            """
          }
        }
      } // globalWraps
